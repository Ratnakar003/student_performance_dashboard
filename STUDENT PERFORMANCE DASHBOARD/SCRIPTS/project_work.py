# -*- coding: utf-8 -*-
"""project work.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Zg15_g7UbzuAARBNjcTv-RsjlXxE16YJ
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

import pandas as pd
df = pd.read_csv("StudentsPerformance.csv")
df.head()

df.info()
df.describe()
df.isnull().sum()

# One-hot encoding for categorical variables
df_encoded = pd.get_dummies(df, columns=['gender', 'race/ethnicity', 'parental level of education', 'lunch', 'test preparation course'], drop_first=True)

# Display the updated dataset
print(df_encoded.head())

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

# Scale the scores (math, reading, writing)
df_encoded[['math score', 'reading score', 'writing score']] = scaler.fit_transform(df_encoded[['math score', 'reading score', 'writing score']])

# Display the scaled dataset
print(df_encoded.head())

# Convert columns to float (if needed)
# Check for missing or infinite values
print(df_encoded.isnull().sum())  # Check for missing values
print(np.isinf(df_encoded).sum())  # Check for infinite values
# Check data types
print(df_encoded.dtypes)



df_encoded[['math score', 'reading score', 'writing score']] = df_encoded[['math score', 'reading score', 'writing score']].astype(float)
# Fill missing values with the mean (or use a more appropriate strategy)
df_encoded.fillna(df_encoded.mean(), inplace=True)

# Define features (X) and target (y)
X = df_encoded.drop(columns=['test preparation course_none'])  # Drop the target variable
y = df_encoded['test preparation course_none']  # Target: whether the student completed the test prep course

# Split into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the Logistic Regression model
from sklearn.linear_model import LogisticRegression
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train, y_train)

# Make predictions
y_pred = log_reg.predict(X_test)

# Evaluate the model
from sklearn.metrics import accuracy_score, classification_report
print("Logistic Regression Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

from sklearn.linear_model import LogisticRegression

# Initialize and train the Logistic Regression model
log_reg = LogisticRegression(max_iter=1000)  # Increased max_iter in case convergence is slow
log_reg.fit(X_train, y_train)

# Make predictions
y_pred = log_reg.predict(X_test)

# Evaluate the model
from sklearn.metrics import accuracy_score, classification_report

# Print the accuracy score and classification report
print("Logistic Regression Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

from sklearn.metrics import confusion_matrix

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print(conf_matrix)

# Visualize confusion matrix (optional)
import seaborn as sns
import matplotlib.pyplot as plt

sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Completed', 'Completed'], yticklabels=['Not Completed', 'Completed'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

import shap
import numpy as np

# Initialize SHAP explainer with correct parameter
explainer = shap.LinearExplainer(log_reg, X_train, feature_perturbation="interventional")

# Compute SHAP values
shap_values = explainer.shap_values(X_test)

# For binary classification, get SHAP values for class 1
if isinstance(shap_values, list):
    shap_values_arr = np.array(shap_values[0], dtype=np.float64)
else:
    shap_values_arr = np.array(shap_values, dtype=np.float64)

# Clean NaN or Inf values
shap_values_arr = np.nan_to_num(shap_values_arr)

# SHAP summary plot
shap.summary_plot(shap_values_arr, X_test)